{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "import xarray as xr\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sn\n",
    "# import statsmodels.api as sm\n",
    "# from ecgdetectors import Detectors\n",
    "# from petastorm import make_reader\n",
    "# from sklearn.metrics import auc,roc_curve,precision_recall_curve,roc_auc_score,RocCurveDisplay\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from sklearn.model_selection import (RepeatedStratifiedKFold, cross_val_score,\n",
    "#                                      train_test_split)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "import shared_utils.utils_data as utils_data\n",
    "from shared_utils import Logistic_reg_model\n",
    "from metrics.main import save_metrics_to_xarray\n",
    "\n",
    "path_formatted_glasgow = \"/workspaces/maitrise/data/20221006_physio_quality/set-a/dataParquet\"\n",
    "path_petastorm = f\"file:///{path_formatted_glasgow}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/workspaces/ecg_evaluation/results\"\n",
    "name_method = [\"Corr_interlead\",\"Corr_intralead\",\"wPMF\",\"SNRECG\",\"HR\",\"Kurtosis\",\"Flatline\",\"TSD\"]\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "os.path.join(save_path,\"\")\n",
    "if not \"quality_metrics.nc\" in os.listdir(save_path):\n",
    "    print(\"Computing metrics\")\n",
    "    if not \"ecg_data.nc\" in os.listdir(save_path):\n",
    "        ds_data = utils_data.format_data_to_xarray(path_petastorm, save_path)\n",
    "    else:\n",
    "        ds_data = xr.load_dataset(os.path.join(save_path,\"ecg_data.nc\"))\n",
    "\n",
    "    ds_metrics = save_metrics_to_xarray(ds_data, name_method, save_path, verbose = True)\n",
    "else:\n",
    "    ds_metrics = xr.load_dataset(os.path.join(save_path,\"quality_metrics.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filtered = ds_metrics.where(ds_metrics.data_quality != \"unlabeled\").dropna(dim = \"id\")\n",
    "\n",
    "np_metrics = ds_filtered.quality_metrics.values\n",
    "metrics_names = ds_filtered.metric_name.values.tolist()\n",
    "np_label = ds_filtered.data_quality.values\n",
    "##Opposite labelling : instead of labelling 1 as acceptable, we label 1 as unacceptable : \n",
    "opposite = True\n",
    "reverseUNO_y = np_label.copy()\n",
    "original_label = np_label.copy()\n",
    "reverseUNO_y[np_label == \"acceptable\" ] = 0\n",
    "reverseUNO_y[np_label == \"unacceptable\" ] = 1\n",
    "reverseUNO_y = reverseUNO_y.astype(int)\n",
    "original_label[np_label == \"acceptable\" ] = 1\n",
    "original_label[np_label == \"unacceptable\" ] = 0\n",
    "original_label = original_label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_index = metrics_names.index(\"HR\")\n",
    "HR_metrics = np_metrics[:,:,HR_index].min(axis=1)\n",
    "X = np_metrics.mean(axis = 1)\n",
    "X[:,HR_index] = HR_metrics\n",
    "df_X = pd.DataFrame(X, columns =metrics_names )\n",
    "df_y_normal = pd.DataFrame(original_label, columns = [\"y\"])\n",
    "df_y_reverse = pd.DataFrame(reverseUNO_y, columns = [\"y\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, Let's create all the folder necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/workspaces/maitrise/results\"\n",
    "column = [\"Corr_interlead\",\"HR\",\"SNRECG\",\"Corr_intralead\"]\n",
    "Logistic_reg_model.save_model_index(df_X,df_y_normal,save_path,cols = column,Model_name = \"JMI_MI_selection_method\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder Format : \n",
    "\n",
    "- Indexes : Only a Folder name Fold_CV with each Test fold results (call Test_Fold_k.csv with k the fold index)\n",
    "- Models : \n",
    "    </br>   \n",
    "    |-name_model.sav\n",
    "    </br>\n",
    "    |-Fold_CV (same orgnization than indexes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV format :\n",
    "\n",
    "- Indexes : [Len_test_fold*3] with 1st column = Proba_label_0, 2nd column = Proba_label_1, 3r column = Ref_test_label\n",
    "- Models : [Len_test_fold*4] with 1st column = Proba_label_0, 2nd column = Proba_label_1, 3rd column = predicted_model_test_label, 4th = ref_test_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Performance Model : \n",
    "\n",
    "If you want to calculate the performance of your model, use \"Classification_report_model\" with the following input :\n",
    "- path toward the folder which contain all your models\n",
    "- name of your model (format of a list in case you want to test mutlitple model at a time)\n",
    "- a threshold argment (use T as a name variable). if not given, all the models performance will be calculated using max MCC threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/workspaces/maitrise/results/Models\"\n",
    "model_s = [\"JMI_MI_selection_method\"]\n",
    "Logistic_reg_model.Classification_report_model(model_path,model_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Performance Index : \n",
    "\n",
    "For the indexes, follow the same principle than before by using \"Classification_report_index\" (input in the same format as before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = \"/workspaces/maitrise/results/Indexes\"\n",
    "index_s = [\"Corr_interlead\"]\n",
    "Logistic_reg_model.Classification_report_index(index_path,index_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ROC PR curve of your model : \n",
    "\n",
    "This time, use \"ROC_PR_CV_curve_model\" with the following inputs:\n",
    "- path toward the folder which contain all your models\n",
    "- name of your model (format of a list in case you want to test mutlitple model at a time)\n",
    "- pos_label = label from which you ant to calculate the performance (int :0 or 1, default =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/workspaces/maitrise/results/Models\"\n",
    "model_s = [\"SQA_method\"]\n",
    "Logistic_reg_model.ROC_PR_CV_curve_model(model_path,model_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ROC PR curve of your index(es) : \n",
    "\n",
    "Same input format as before instead you use \"ROC_PR_CV_curve_indexes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = \"/workspaces/maitrise/results/Indexes\"\n",
    "index_s = [\"wPMF\"]\n",
    "Logistic_reg_model.ROC_PR_CV_curve_index(index_path,index_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative ROC PR Curve for all indexes and models : \n",
    "for that, use \"Global_comp_ROC_PR_mean_curve\" with the following inputs :\n",
    "- path toward the folder which contain all your models\n",
    "- path toward the folder which contain all your indexes\n",
    "- name of your model (format of a list in case you want to test mutlitple model at a time) (default empty list)\n",
    "- name of your indexes (format of a list in case you want to test mutlitple indexes at a time) (default : empty list)\n",
    "- pos_label = label from which you ant to calculate the performance (int :0 or 1, default =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic_reg_model.Global_comp_ROC_PR_mean_curve(model_path,index_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 23 2023, 22:32:48) [GCC 10.2.1 20210110]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e9b5c13aa136530306b9d0fda952fcd75969540fafdedb30f13be0697230024"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
