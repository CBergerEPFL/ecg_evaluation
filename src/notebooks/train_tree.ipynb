{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from IPython.display import clear_output\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from scipy.misc import derivative\n",
    "from sklearn.metrics import (accuracy_score, classification_report, f1_score,\n",
    "                             log_loss, roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "import shared_utils.utils_data as utils_data\n",
    "from Metrics.Wrapper_main_function import (compute_metrics,\n",
    "                                           save_metrics_to_xarray)\n",
    "\n",
    "path_formatted_glasgow = \"/workspaces/maitrise/data/20221006_physio_quality/set-a/dataParquet\"\n",
    "path_petastorm = f\"file:///{path_formatted_glasgow}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1./(1. +  np.exp(-x))\n",
    "\n",
    "def focal_loss_lgb(y_pred, dtrain, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Focal Loss for lightgbm\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred: numpy.ndarray\n",
    "        array with the predictions\n",
    "    dtrain: lightgbm.Dataset\n",
    "    alpha, gamma: float\n",
    "        See original paper https://arxiv.org/pdf/1708.02002.pdf\n",
    "    \"\"\"\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    def fl(x,t):\n",
    "        p = 1/(1+np.exp(-x))\n",
    "        return -( a*t + (1-a)*(1-t) ) * (( 1 - ( t*p + (1-t)*(1-p)) )**g) * ( t*np.log(p)+(1-t)*np.log(1-p) )\n",
    "    partial_fl = lambda x: fl(x, y_true)\n",
    "    grad = derivative(partial_fl, y_pred, n=1, dx=1e-6)\n",
    "    hess = derivative(partial_fl, y_pred, n=2, dx=1e-6)\n",
    "    return grad, hess\n",
    "\n",
    "def lgb_focal_f1_score(preds, lgbDataset):\n",
    "    \"\"\"\n",
    "    When using custom losses the row prediction needs to passed through a\n",
    "    sigmoid to represent a probability\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    preds: numpy.ndarray\n",
    "        array with the predictions\n",
    "    lgbDataset: lightgbm.Dataset\n",
    "    \"\"\"\n",
    "    preds = sigmoid(preds)\n",
    "    \n",
    "    binary_preds = [int(p>0.5) for p in preds]\n",
    "    y_true = lgbDataset.get_label()\n",
    "    return 'f1', f1_score(y_true, binary_preds), True\n",
    "\n",
    "def focal_loss_lgb_eval_error(y_pred, dtrain, alpha, gamma):\n",
    "  a,g = alpha, gamma\n",
    "  y_true = dtrain.label\n",
    "  p = 1/(1+np.exp(-y_pred))\n",
    "  loss = -( a*y_true + (1-a)*(1-y_true) ) * (( 1 - ( y_true*p + (1-y_true)*(1-p)) )**g) * ( y_true*np.log(p)+(1-y_true)*np.log(1-p) )\n",
    "  # (eval_name, eval_result, is_higher_better)\n",
    "  return 'focal_loss', np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_name = \"binary_logloss\"\n",
    "# metric_name = 'auc'\n",
    "focal_loss = lambda x,y: focal_loss_lgb(x, y,alpha=0.25, gamma=2 )\n",
    "focal_loss_error = lambda y_pred, dtrain: focal_loss_lgb_eval_error(y_pred, dtrain, alpha=0.25, gamma=2)\n",
    "metric_name = \"focal_loss\"\n",
    "param_fixed = { \n",
    "        \"metric\": metric_name,  \n",
    "        # \"is_unbalance\": True,\n",
    "        \"verbosity\": -1\n",
    "        }\n",
    "        \n",
    "def objective(trial, X, y):\n",
    "    \n",
    "    d_train = lgb.Dataset(X, label=y)\n",
    "    param_grid = {\n",
    "        # \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10,50,100,500, 1000]),\n",
    "        \"learning_rate\":  trial.suggest_loguniform('learning_rate', 0.01, 0.5),\n",
    "        # \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 1000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 20),\n",
    "        # \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        # \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95),\n",
    "        'feature_fraction': trial.suggest_float(\"feature_fraction\", 0.2, 0.95)\n",
    "    }\n",
    "    param = param_fixed | param_grid\n",
    "\n",
    "    lcv = lgb.cv(\n",
    "        param, \n",
    "        d_train, \n",
    "        callbacks = [lgb.early_stopping(30), \n",
    "                            # lgb.log_evaluation(0)\n",
    "                            ], \n",
    "                            num_boost_round =300,\n",
    "        fobj = focal_loss,\n",
    "        feval = focal_loss_error,\n",
    "        # verbose_eval=False\n",
    "        )\n",
    "    return lcv[f\"{metric_name}-mean\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/workspaces/maitrise/results\"\n",
    "name_method = [\"Corr_interlead\",\"Corr_intralead\",\"wPMF\",\"SNRECG\",\"HR\",\"Kurtosis\",\"Flatline\",\"TSD\"]\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "if not \"quality_metrics.nc\" in os.listdir(save_path):\n",
    "    print(\"Computing metrics\")\n",
    "    if not \"ecg_data.nc\" in os.listdir(save_path):\n",
    "        ds_data = utils_data.format_data_to_xarray(path_petastorm, save_path)\n",
    "    else:\n",
    "        ds_data = xr.load_dataset(os.path.join(save_path,\"ecg_data.nc\"))\n",
    "\n",
    "    ds_metrics = save_metrics_to_xarray(ds_data, name_method, save_path, verbose = True)\n",
    "else:\n",
    "    ds_metrics = xr.load_dataset(os.path.join(save_path,\"quality_metrics.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filtered = ds_metrics.where(ds_metrics.data_quality != \"unlabeled\").dropna(dim = \"id\")\n",
    "\n",
    "np_metrics = ds_filtered.quality_metrics.values\n",
    "metrics_names = ds_filtered.metric_name.values.tolist()\n",
    "np_label = ds_filtered.data_quality.values\n",
    "np_label[np_label == \"acceptable\" ] = 0\n",
    "np_label[np_label == \"unacceptable\" ] = 1\n",
    "np_label = np_label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((np_metrics.min(axis = 1),np_metrics.mean(axis = 1)),axis =-1)\n",
    "metric_name_merged = [f\"{x}_min\" for x in metrics_names] + [f\"{x}_mean\" for x in metrics_names]\n",
    "df_X = pd.DataFrame(X, columns =metric_name_merged )\n",
    "y = np_label\n",
    "df_y = pd.DataFrame(np_label, columns = [\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(df_X, y, test_size=0.2, random_state=1234)\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n",
    "func = lambda trial: objective(trial, train_x, train_y)\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study.optimize(func, n_trials=100)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\tBest value binary_logloss: {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "train2_x, val_x, train2_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=1234)\n",
    "d_train = lgb.Dataset(train2_x, label=train2_y)\n",
    "d_val = lgb.Dataset(val_x, label=val_y)\n",
    "param = param_fixed | study.best_params\n",
    "gbm = lgbm.train(param, \n",
    "            d_train, valid_sets = d_val, \n",
    "            callbacks = [lgb.early_stopping(30), \n",
    "                            lgb.log_evaluation(0)], \n",
    "            fobj = focal_loss,\n",
    "            feval = focal_loss_error,      \n",
    "                            )\n",
    "\n",
    "# np.save(os.path.join(path_results,'test_set_x'), test_x.values)\n",
    "# np.save(os.path.join(path_results,'test_set_y'), test_y)\n",
    "\n",
    "preds = gbm.predict(test_x)\n",
    "preds = sigmoid(preds)\n",
    "pred_labels = (preds > 0.5).astype('int')\n",
    "score = classification_report(test_y, pred_labels)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(test_y, pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = [\"acceptable\", \"unacceptable\"])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve,auc\n",
    "print(roc_auc_score(test_y, preds))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_y, preds)\n",
    "prec,rec,_ = precision_recall_curve(test_y,preds)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr,label=\"AUC = {:.2f}\".format(roc_auc_score(test_y, preds)))\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend(loc = \"best\")\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rec, prec,label=\"AUC = {:.2f}\".format(auc(rec,prec)))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid()\n",
    "plt.legend(loc = \"best\")\n",
    "plt.title('PR curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with pickle\n",
    "# with open(os.path.join(path_results,'lgb_classifier.pkl'), 'wb') as fout:\n",
    "#     pickle.dump(gbm, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 15 2022, 20:55:06) [GCC 10.2.1 20210110]"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
