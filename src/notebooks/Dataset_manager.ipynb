{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "from petastorm.unischema import Unischema,UnischemaField\n",
    "from petastorm.codecs import ScalarCodec,NdarrayCodec\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "from operations.dataset_manager import get_dataset,get_name_files,save_to_parquet_petastorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get your dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"mit-bih-noise-stress-test-database-1.0.0\"\n",
    "ignore_folder = False\n",
    "files_name = get_name_files(name,ignore_inner_folder=ignore_folder)\n",
    "dataset = get_dataset(name,fs=500,time_window=10,ignore_subdfolder=ignore_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data parquet file (the parquet scheme will be given)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking your cpu core and usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check number of cpu core and memory\n",
    "print(os.cpu_count())\n",
    "# Getting % usage of virtual_memory ( 3rd field)\n",
    "print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "# Getting usage of virtual_memory in GB ( 4th field)\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ceating a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('MyfirstSpark').master('local[2]').config('spark.driver.memory', '4g').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unischema use (This is the one we use! please provide your unischema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECGSchemaPhysio = Unischema(\n",
    "    \"ECGSchema\",\n",
    "    [\n",
    "        UnischemaField(\"noun_id\", np.string_, (), ScalarCodec(StringType()), False),\n",
    "        UnischemaField(\"signal\", np.float64, (None,None,None), NdarrayCodec(), False),\n",
    "        UnischemaField(\"fs\", np.int_, (), ScalarCodec(IntegerType()), False),\n",
    "        UnischemaField(\"sig_len\", np.int_, (), ScalarCodec(IntegerType()), False),\n",
    "        UnischemaField(\"sig_name\", np.string_, (None,), NdarrayCodec(), False),\n",
    "        UnischemaField(\"n_sig\", np.int_, (), ScalarCodec(IntegerType()), False),\n",
    "        UnischemaField(\"base_date\", np.string_, (), ScalarCodec(StringType()), True),\n",
    "        UnischemaField(\"base_time\", np.string_, (), ScalarCodec(StringType()), True),\n",
    "        UnischemaField(\"units\", np.string_, (None,), NdarrayCodec(), False),\n",
    "        UnischemaField(\"comments\", np.string_, (None,), NdarrayCodec(), False),\n",
    "        UnischemaField(\"nb_time_window\", np.int_, (), ScalarCodec(IntegerType()), False),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset into parquet file (the folder containing the files will be in your Physionet folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a row generator (return a single entry of the dataset. This is the one we use. You can modify it, if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_generator(x):\n",
    "    \"\"\"Returns a single entry in the generated dataset.\"\"\"\n",
    "    return dataset[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset in parquet file (Warning: Depending on the size of your dataset, this process can take time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_parquet_petastorm(dataset, name, spark, ECGSchemaPhysio, row_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
